{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Smart Bag - Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIi7Gk_VgYyM",
        "outputId": "3dc96feb-190b-4e7f-938f-7e99f22d11e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow_recommenders"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_recommenders\n",
            "  Downloading tensorflow_recommenders-0.6.0-py3-none-any.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow_recommenders) (1.0.0)\n",
            "Requirement already satisfied: tensorflow>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_recommenders) (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.1.6->tensorflow_recommenders) (1.15.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (4.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (3.17.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (0.5.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (57.4.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (14.0.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (1.21.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (1.46.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.6.0->tensorflow_recommenders) (0.26.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.6.0->tensorflow_recommenders) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.6.0->tensorflow_recommenders) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.6.0->tensorflow_recommenders) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, tensorflow-recommenders\n",
            "Successfully installed tensorflow-recommenders-0.6.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-kYYYj8thQv"
      },
      "source": [
        "# Essential imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from typing import Dict, Text\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "import tensorflow_datasets as tfds\n",
        "import tempfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBEVNxwt72s_",
        "outputId": "5f42278f-124e-4ac0-a8e6-e558cc0f31f3"
      },
      "source": [
        "# Mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "LqWy_PE6Qh1F",
        "outputId": "81463cc5-abff-4c24-8e65-e05c65af8b74"
      },
      "source": [
        "# Loading the orders data\n",
        "ords = pd.read_csv('drive/My Drive/Flipkart Grid/orders.csv').drop(columns = ['Unnamed: 0'])\n",
        "ords.head()u"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c0e2792ab34d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading the orders data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/My Drive/Flipkart Grid/orders.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/Flipkart Grid/orders.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1J-oph7Nd6I"
      },
      "source": [
        "ords.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41CYoQ-yh66D"
      },
      "source": [
        "# Converting the ID indice to string format, as will be required by our model\n",
        "ords['Order ID'] = ords['Order ID'].astype(str)\n",
        "ords['User'] = ords['User'].astype(str)\n",
        "ords['Item ID'] = ords['Item ID'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCGtnqn0hjhb"
      },
      "source": [
        "print(ords.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr_6U012RpGE"
      },
      "source": [
        "# Loading the products table\n",
        "products = pd.read_csv('drive/My Drive/Flipkart Grid/products.csv').drop(columns = ['Unnamed: 0'])\n",
        "products.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAP__EAQhtdN"
      },
      "source": [
        "# Converting the Item Id to string format here as well\n",
        "products['Item ID'] = products['Item ID'].astype(str)\n",
        "print(products.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_SvhC-MsxLJ"
      },
      "source": [
        "# Creating a TF data set out of the orders table, for passing into our model\n",
        "orders = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(ords['Item ID'].values, tf.string),\n",
        "            tf.cast(ords['User'].values, tf.string),\n",
        "            tf.cast(ords['Day of Month'].values, tf.int16)\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uugo1V-SuVDN"
      },
      "source": [
        "# Creating a TF data set out of the products table, for passing into our model\n",
        "prods = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(products['Item ID'].values, tf.string)\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IwuLcYztd38"
      },
      "source": [
        "type(orders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW4OpGqvNUU3"
      },
      "source": [
        "# Splitting into train and test sets after shuffling\n",
        "tf.random.set_seed(42)\n",
        "shuffled = orders.shuffle(18000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(15000)\n",
        "test = shuffled.skip(15000).take(3000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ-KYBwdNoFE"
      },
      "source": [
        "print(len(train))\n",
        "print(len(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N4genMFODer"
      },
      "source": [
        "# Generating lists of unique product names (IDs) and user names (IDs) \n",
        "product_names = prods.batch(300)\n",
        "user_ids = orders.batch(18000).map(lambda x, y, z: y)\n",
        "\n",
        "unique_products = np.unique(np.concatenate(list(product_names)))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
        "\n",
        "print(unique_products[:10])\n",
        "print(unique_user_ids[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX5A-IiXOk55"
      },
      "source": [
        "# Defining the embedding dimensions\n",
        "embedding_dimension = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCQT5o4IOuvP"
      },
      "source": [
        "# Creating the Query tower\n",
        "# The StringLookup layer is creating a vocabuary of user IDs\n",
        "# A provision to handle new IDs is also present\n",
        "# The embedding layer would learn a unique representation for each user\n",
        "user_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "      vocabulary=unique_user_ids, mask_token=None),\n",
        "  # We add an additional embedding to account for unknown tokens.\n",
        "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5kDQARhO1-b"
      },
      "source": [
        "# Creating the Candidate tower\n",
        "# The StringLookup layer is creating a vocabuary of item IDs\n",
        "# A provision to handle new IDs is also present\n",
        "# The embedding layer would learn a unique representation for each item\n",
        "product_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "      vocabulary=unique_products, mask_token=None),\n",
        "  tf.keras.layers.Embedding(len(unique_products) + 1, embedding_dimension)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvGLicdWO_0S"
      },
      "source": [
        "# Defining the metric\n",
        "metrics = tfrs.metrics.FactorizedTopK(\n",
        "  candidates=prods.batch(128).map(product_model)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5wPnkOaPEk-"
      },
      "source": [
        "# Defining the loss\n",
        "task = tfrs.tasks.Retrieval(\n",
        "  metrics=metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjIePw-RPIzc"
      },
      "source": [
        "# Creating a model that combines both the user and product models\n",
        "class productlensModel(tfrs.Model):\n",
        "\n",
        "  def __init__(self, user_model, product_model):\n",
        "    super().__init__()\n",
        "    self.product_model: tf.keras.Model = product_model\n",
        "    self.user_model: tf.keras.Model = user_model\n",
        "    self.task: tf.keras.layers.Layer = task\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    # We pick out the user features and pass them into the user model.\n",
        "    user_embeddings = self.user_model(features[1])\n",
        "    # And pick out the product features and pass them into the product model,\n",
        "    # getting embeddings back.\n",
        "    positive_product_embeddings = self.product_model(features[0])\n",
        "\n",
        "    # The task computes the loss and the metrics.\n",
        "    return self.task(user_embeddings, positive_product_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lpPVyZwPo0o"
      },
      "source": [
        "# Defining the final model that implements a custom training loop\n",
        "# This is an alternative to the one defined above\n",
        "# It has been provided to better explain what the training looks like\n",
        "# However, we have used the previous model finally\n",
        "class NoBaseClassproductlensModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, user_model, product_model):\n",
        "    super().__init__()\n",
        "    self.product_model: tf.keras.Model = product_model\n",
        "    self.user_model: tf.keras.Model = user_model\n",
        "    self.task: tf.keras.layers.Layer = task\n",
        "\n",
        "  def train_step(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
        "\n",
        "    # Set up a gradient tape to record gradients.\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      # Loss computation.\n",
        "      user_embeddings = self.user_model(features[1])\n",
        "      positive_product_embeddings = self.product_model(features[0])\n",
        "      loss = self.task(user_embeddings, positive_product_embeddings)\n",
        "\n",
        "      # Handle regularization losses as well.\n",
        "      regularization_loss = sum(self.losses)\n",
        "\n",
        "      total_loss = loss + regularization_loss\n",
        "\n",
        "    gradients = tape.gradient(total_loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    metrics = {metric.name: metric.result() for metric in self.metrics}\n",
        "    metrics[\"loss\"] = loss\n",
        "    metrics[\"regularization_loss\"] = regularization_loss\n",
        "    metrics[\"total_loss\"] = total_loss\n",
        "\n",
        "    return metrics\n",
        "\n",
        "  def test_step(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
        "\n",
        "    # Loss computation.\n",
        "    user_embeddings = self.user_model(features[1])\n",
        "    positive_product_embeddings = self.product_model(features[0])\n",
        "    loss = self.task(user_embeddings, positive_product_embeddings)\n",
        "\n",
        "    # Handle regularization losses as well.\n",
        "    regularization_loss = sum(self.losses)\n",
        "\n",
        "    total_loss = loss + regularization_loss\n",
        "\n",
        "    metrics = {metric.name: metric.result() for metric in self.metrics}\n",
        "    metrics[\"loss\"] = loss\n",
        "    metrics[\"regularization_loss\"] = regularization_loss\n",
        "    metrics[\"total_loss\"] = total_loss\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re7Gq62-R5gS"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In9-nnzmRtxN"
      },
      "source": [
        "# Defining the following callbacks for training\n",
        "# ModelCheckpoint - Will save the best model seen up till any epoch\n",
        "# ReduceLROnPlateau - Will reduce the learning rate by a factor of 10 if the loss hasn't reduced for 4 epochs\n",
        "# EarlyStppoing - Stops training if the loss doesn't improve for 6 epochs\n",
        "# Tensorboard - Stores the training logs to be plotted later\n",
        "!rm -rf ./logs/\n",
        "from datetime import datetime\n",
        "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=0)\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint('./best_model', save_weights_only=True, save_best_only=True, mode='min', monitor='total_loss', save_format = 'tf'),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='total_loss', min_lr=0.000001,patience=4),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='total_loss', patience=6, mode='min', baseline=None, restore_best_weights=False),\n",
        "    tensorboard_callback\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUw7ShNyP_8G"
      },
      "source": [
        "# Compiling the model\n",
        "model = productlensModel(user_model, product_model)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7LcQMzbQLMl"
      },
      "source": [
        "# Creating data generators for the train and test data\n",
        "cached_train = train.shuffle(15000).batch(64).cache()\n",
        "cached_test = test.batch(64).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDqrN7ZOQYo6"
      },
      "source": [
        "# Fitting the model\n",
        "model.fit(cached_train, epochs=30, callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vujnN5m5THJt"
      },
      "source": [
        "# Plotting the tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrMfQCI5bXll"
      },
      "source": [
        "All the metrics are very promising. Let us check the performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIAi5wsEQmQK"
      },
      "source": [
        "model.evaluate(cached_test, return_dict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcSPjc0WbgC0"
      },
      "source": [
        "The test metrics are understandable much lower, but this doesn't mean the performance is bad. We can verify this from our observations below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y285oFyQyYa"
      },
      "source": [
        "# Creating a model that takes in raw query features, and\n",
        "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
        "# recommending movies out of the entire movies dataset.\n",
        "index.index_from_dataset(\n",
        "  tf.data.Dataset.zip((prods.batch(100), prods.batch(100).map(model.product_model)))\n",
        ")\n",
        "user_number = \"777\"\n",
        "# Getting recommendations.\n",
        "_, titles = index(tf.constant([user_number]))\n",
        "print(f\"Recommendations for user {user_number}: {titles[0, :10]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyA3I7SrRUwb"
      },
      "source": [
        "# Printing the product type and name for the top 10 predictions\n",
        "rec_types, rec_names = [], []\n",
        "for rec in titles[0]:\n",
        "  print(products[products['Item ID'] == np.array(rec).astype(str)].values[0][3:5])\n",
        "  rec_types.append(products[products['Item ID'] == np.array(rec).astype(str)].values[0][3])\n",
        "  rec_names.append(products[products['Item ID'] == np.array(rec).astype(str)].values[0][4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Ah3rpFRaEx"
      },
      "source": [
        "# Printing the types and names of all ordered products for the concerned user\n",
        "ord_types, ord_names = [], []\n",
        "for _, _, item, _ in ords[ords['User'] == user_number].values:\n",
        "  print(products[products['Item ID'] == item].values[0][3:5])\n",
        "  ord_types.append(products[products['Item ID'] == item].values[0][3])\n",
        "  ord_names.append(products[products['Item ID'] == item].values[0][4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m96fZC7tcFA1"
      },
      "source": [
        "Excellent! All products that have been ordered by this user are being recommended!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QA3PKD2TuiW"
      },
      "source": [
        "# Getting Top 3 Recommendations\n",
        "for i in range(3):\n",
        "  print(\"ID: \", products[products.Name == rec_names[i]].values[0][0], \" | Name: \", rec_names[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqPYnqqEWxcQ"
      },
      "source": [
        "# Most frequently ordered products\n",
        "user_data = ords[ords.User == user_number]\n",
        "frequent = user_data['Item ID'].value_counts().index.tolist()[:2]\n",
        "values = list(user_data['Item ID'].value_counts())\n",
        "for i in range(2):\n",
        "  print(\"ID: \", frequent[i], \" | Name: \", products[products['Item ID'] == frequent[i]].values[0][4], \" | No. of purchases: \", values[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciHAh-dsZCVk"
      },
      "source": [
        "# Bigger/Discounted/Sponosored products\n",
        "user_data = ords[ords.User == user_number]\n",
        "user_data = user_data.merge(products, on = 'Item ID')\n",
        "freq_types = user_data['Type'].value_counts().index.tolist()[:3]\n",
        "discounted = []\n",
        "bigger = []\n",
        "sponsored = []\n",
        "for ptype in freq_types:\n",
        "  print('Category = ', ptype)\n",
        "  subset = products[products['Type'] == ptype]\n",
        "  disc = subset[subset['Discount'] == subset['Discount'].value_counts().index.tolist()[0]].values[0]\n",
        "  biggest = subset[subset['Quantity'] == subset['Quantity'].value_counts().index.tolist()[0]].values[0]\n",
        "  spons = subset[subset['Sponsored'] == True].values\n",
        "  print(\"Discounted product -> \", end = \" \")\n",
        "  if disc[-3]:\n",
        "    print(\"ID: \", disc[0], \" | Name: \", disc[4], \" | Discount: \", disc[-3], \"%\")\n",
        "    discounted.append(disc[0])\n",
        "  else:\n",
        "    print('No discount for type ', ptype)\n",
        "  print(\"Largest Product in Category -> \", end = \" \")\n",
        "  print(\"ID: \", biggest[0], \" | Name: \", biggest[4], \" | Quantity: \", disc[-2])\n",
        "  bigger.append(biggest)\n",
        "  if len(spons) > 0:\n",
        "    print(\"Sponsored product -> \", end = \" \")\n",
        "    print(\"ID: \", spons[0][0], \" | Name: \", spons[0][4], \" | Discount: \", disc[-3], \"%\")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2AmWqJdTudA"
      },
      "source": [
        "# Exporting the query model.\n",
        "with tempfile.TemporaryDirectory() as tmp:\n",
        "  path = 'drive/My Drive/Flipkart Grid/TFRS_model'\n",
        "\n",
        "  # Saving the index.\n",
        "  tf.saved_model.save(index, path)\n",
        "\n",
        "  # Loading it back; can also be done in TensorFlow Serving.\n",
        "  loaded = tf.saved_model.load(path)\n",
        "\n",
        "  # Passing a user id in, getting top predicted movie titles back.\n",
        "  scores, titles = loaded([user_number])\n",
        "\n",
        "  print(f\"Recommendations: {titles[0][:10]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oBkKBZdUeug"
      },
      "source": [
        "# Getting recommendations.\n",
        "print(\"The user {} is in our records: {}\". format(\"1050\", len(ords[ords['User'] == \"1050\"]) > 0))\n",
        "_, titles = index(tf.constant([\"1050\"]))\n",
        "print(f\"Recommendations for new user: {titles[0, :10]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N75kQ9HlUYSi"
      },
      "source": [
        "# Printing recommendations for new users\n",
        "for rec in titles[0]:\n",
        "  print(products[products['Item ID'] == np.array(rec).astype(str)].values[0][3:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGvrOVFJFrIv"
      },
      "source": [
        "# Getting the most purchased products by all users\n",
        "ords[['Order ID', 'Item ID']].groupby(by = 'Item ID').count().sort_values(by = 'Order ID', ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS9QLFvZGN_c"
      },
      "source": [
        "# Printing the top 10 most frequently purchased products along with the frequenies\n",
        "pop = ords[['Order ID', 'Item ID']].groupby(by = 'Item ID').count().sort_values(by = 'Order ID', ascending = False)\n",
        "popular = pop.index.tolist()[:10]\n",
        "values = pop.values[:10]\n",
        "for i, item in enumerate(popular):\n",
        "  print(\"ID: \", item, \" | Name: \", products[products['Item ID'] == item].values[0][4], \" | No. of purchases: \", values[i][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL_kP30mH1ID"
      },
      "source": [
        "# Prediction function\n",
        "def SmartBagRecommendations(user_number):\n",
        "  with tempfile.TemporaryDirectory() as tmp:\n",
        "    path = 'drive/My Drive/Flipkart Grid/TFRS_model'\n",
        "\n",
        "    # Load it back; can also be done in TensorFlow Serving.\n",
        "    loaded = tf.saved_model.load(path)\n",
        "\n",
        "    # Pass a user id in, get top predicted movie titles back.\n",
        "    scores, titles = loaded([user_number])\n",
        "    \n",
        "    rec_ids = []\n",
        "    for rec in titles[0]:\n",
        "      rec_ids.append(products[products['Item ID'] == np.array(rec).astype(str)].values[0][0])\n",
        "\n",
        "    recommendations = {}\n",
        "\n",
        "    if len(ords[ords['User'] == user_number]) > 0:\n",
        "      # Initializing the dictionary of labeled predictions\n",
        "\n",
        "      # Adding model's top 10 predictions to the 'top' list\n",
        "      recommendations['top'] = rec_ids\n",
        "\n",
        "      # Adding the 2 most frequently ordered products\n",
        "      user_data = ords[ords.User == user_number]\n",
        "      frequent = user_data['Item ID'].value_counts().index.tolist()[:3]\n",
        "      values = list(user_data['Item ID'].value_counts())\n",
        "      recommendations['most_bought'] = frequent\n",
        "\n",
        "      # Adding discount/larger/sponsored product recommendations\n",
        "      recommendations['discounted'], recommendations['bigger'], recommendations['sponsored'] = [], [], []\n",
        "      user_data = user_data.merge(products, on = 'Item ID')\n",
        "      freq_types = user_data['Type'].value_counts().index.tolist()[:3]\n",
        "      for ptype in freq_types:\n",
        "        subset = products[products['Type'] == ptype]\n",
        "        disc = subset[subset['Discount'] == subset['Discount'].value_counts().index.tolist()[0]].values[0]\n",
        "        biggest = subset[subset['Quantity'] == subset['Quantity'].value_counts().index.tolist()[0]].values[0]\n",
        "        spons = subset[subset['Sponsored'] == True].values\n",
        "        if disc[-3]: recommendations['discounted'].append(disc[0])\n",
        "        recommendations['bigger'].append(biggest[0])\n",
        "        if len(spons) > 0: recommendations['sponsored'].append(spons[0][0])\n",
        "\n",
        "    else:\n",
        "      recommendations = {'model_based': rec_ids}\n",
        "      pop = ords[['Order ID', 'Item ID']].groupby(by = 'Item ID').count().sort_values(by = 'Order ID', ascending = False).index.tolist()[:10]\n",
        "      recommendations['data_based'] = pop\n",
        "    return recommendations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsdDTUMOM8Sz"
      },
      "source": [
        "# Getting recommendations for old users\n",
        "%%time\n",
        "SmartBagRecommendations(\"777\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QU3q7W5elu2"
      },
      "source": [
        "Please note that the difference in exact recommended values is because of the fact the a few lines of code were changed in the prediction notebook after the model was trained and stored. We felt it was better to leave this function, instead of running the risks of getting bugs by editing without running the notebook. Running the notebook would mean chaning the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXJVVMUIM8Ya"
      },
      "source": [
        "# Getting recommendations for new users\n",
        "%%time\n",
        "SmartBagRecommendations(\"2000\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}